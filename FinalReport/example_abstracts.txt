The small size of micro aerial vehicles (MAVs) allows a wide range of robotic applications, such as surveil- lance, inspection and search & rescue. In order to operate autonomously, the robot requires the ability to known its position and movement in the environment. Since no assumptions can be made about the en- vironment, the robot has to learn from its environment. Simultaneous Localization and Mapping (SLAM) using aerial vehicles is an active research area in robotics. However, current approaches use algorithms that are computationally expensive and cannot be applied for real-time navigation problems. Furthermore, most researchers rely on expensive aerial vehicles with advanced sensors.
This thesis presents a real-time SLAM approach for affordable MAVs with a down-looking camera. Focusing on real-time methods and affordable MAVs increases the employability of aerial vehicles in real world situations. The approach has been validated with the AR.Drone quadrotor helicopter, which was the standard platform for the International Micro Air Vehicle competition. The development is partly based on simulation, which requires both a realistic sensor and motion model. The AR.Drone simulation model is described and validated.
Furthermore, this thesis describes how a visual map of the environment can be made. This visual map consists of a texture map and a feature map. The texture map is used for human navigation and the feature map is used by the AR.Drone to localize itself. A localization method is presented. It uses a novel approach to robustly recover the translation and rotation between a camera frame and the map. An experimental method to create an elevation map with a single airborne ultrasound sensor is presented. This elevation map is combined with the texture map and visualized in real-time.
Experiments have validated that the presented methods work in a variety of environments. One of the experiments demonstrates how well the localization works for circumstances encountered during the IMAV competition. Furthermore, the impact of the camera resolution and various pose recovery approaches are investigated.



We consider the problem of autonomously flying Miniature Aerial Vehicles (MAVs) in indoor environments such as home and office buildings. The primary long range sensor in these MAVs is a miniature camera. While previous approaches first try to build a 3D model in order to do planning and control, our method neither attempts to build nor requires a 3D model. Instead, our method first classifies the type of indoor environment the MAV is in, and then uses vision algorithms based on perspective cues to estimate the desired direction to fly. We test our method on two MAV platforms: a co-axial miniature helicopter and a toy quadrotor. Our experiments show that our vision algorithms are quite reliable, and they enable our MAVs to fly in a variety of corridors and staircases.

MIT Lincoln Laboratory has expressed growing interest in projects involving Unmanned Aerial Vehicles (UAVs). Recently, they purchased a Cyber Technology CyberQuad quadrotor UAV. Our project’s task was to assist the Laboratory in preparation for the future automation of this system. In particular, this required the creation system allowing computerized-control of the UAV – specifically interfacing with the software tools Lincoln Laboratory’s Group 76 intended use for future development, as well as a high-accuracy localization system to aid with take-off and landing in anticipated mission environments.
We successfully created a computer control interface between the CyberQuad and Willow Garage’s Robot Operating System used at the Laboratory. This interface could send commands to and receive responses from the quadrotor. We tested the performance of the quadrotor using our interface and compared it against the original analog control joystick. Latency and link health tools were developed, and they indicated that our solution, while clearly less responsive than the analog controller, would be usable after minor improvements.
To enable localization we investigated machine vision and video processing libraries, altering the augmented reality library ARToolKit to work with ROS. We performed accuracy, range, update rate, lighting, and tag occlusion tests on our modified code to determine its viability in real-world conditions. Ultimately, we concluded that our current system would not be a feasible alternative to current techniques due to inconsistencies in tag-detection, though the high accuracy and update rate convinced us that this localization method merits future investigation as new software packages become available.

This paper presents the AR-Drone quadrotor helicopter as a robotic platform usable for research and education. Apart from the description of hardware and software, we discuss several issues regarding drone equipment, abilities and performance. We show, how to perform basic tasks of position stabilization, object following and autonomous navigation. Moreover, we demonstrate the drone ability to act as an ex- ternal navigation system for a formation of mobile robots. To further demonstrate the drone utility for robotic research, we describe experi- ments in which the drone has been used. We also introduce a freely avail- able software package, which allows researches and students to quickly overcome the initial problems and focus on more advanced issues.


We present an approach that enables a low-cost quadrocopter to accurately fly various figures using vision as main sensor modality. Our approach consists of three components: a monocular SLAM system, an extended Kalman filter for data fusion and state estimation and a PID controller to generate steering commands. Our system is able to navigate in previously unknown indoor and outdoor environments at absolute scale without requiring artificial markers or external sensors. Next to a full description of our system, we introduce our scripting language and present several examples of accurate figure flying in the corresponding video submission.